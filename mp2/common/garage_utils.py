
"""  Meta-learning utilities borrowed from Garage """


def zero_optim_grads(optim, set_to_none=True):
    """Sets the gradient of all optimized tensors to None.
    This is an optimization alternative to calling `optimizer.zero_grad()`
    Args:
        optim (torch.nn.Optimizer): The optimizer instance
            to zero parameter gradients.
        set_to_none (bool): Set gradients to None
            instead of calling `zero_grad()`which
            sets to 0.
    """
    if not set_to_none:
        optim.zero_grad()
        return

    for group in optim.param_groups:
        for param in group['params']:
            param.grad = None


def update_module_params(module, new_params):  # noqa: D202
    """Load parameters to a module.

    This function acts like `torch.nn.Module._load_from_state_dict()`, but
    it replaces the tensors in module with those in new_params, while
    `_load_from_state_dict()` loads only the value. Use this function so
    that the `grad` and `grad_fn` of `new_params` can be restored

    Args:
        module (torch.nn.Module): A torch module.
        new_params (dict): A dict of torch tensor used as the new
            parameters of this module. This parameters dict should be
            generated by `torch.nn.Module.named_parameters()`

    """
    named_modules = dict(module.named_modules())

    # pylint: disable=protected-access
    def update(m, name, param):
        del m._parameters[name]  # noqa: E501
        setattr(m, name, param)
        m._parameters[name] = param  # noqa: E501

    for name, new_param in new_params.items():
        if '.' in name:
            module_name, param_name = tuple(name.rsplit('.', 1))
            if module_name in named_modules:
                update(named_modules[module_name], param_name, new_param)
        else:
            update(module, name, new_param)


class DifferentiableSGD:
    """
    (From Garage)
    Differentiable Stochastic Gradient Descent

    DifferentiableSGD performs the same optimization step as SGD, but instead
    of updating parameters in-place, it saves updated parameters in new
    tensors, so that the gradient of functions of new parameters can flow back
    to the pre-updated parameters.

    Args:
        module (torch.nn.module): A torch module whose parameters need to be
            optimized.
        lr (float): Learning rate of stochastic gradient descent.

    """

    def __init__(self, module, lr=1e-3):
        self.module = module
        self.lr = lr

    def step(self):
        """Take an optimization step."""
        memo = set()

        def update(module):
            for child in module.children():
                if child not in memo:
                    memo.add(child)
                    update(child)

            params = list(module.named_parameters())
            for name, param in params:
                # Skip descendant modules' parameters.
                if '.' not in name:
                    if param.grad is None:
                        continue

                    # Original SGD uses param.grad.data
                    new_param = param.add(param.grad, alpha=-self.lr)

                    del module._parameters[name]  # pylint: disable=protected-access # noqa: E501
                    setattr(module, name, new_param)
                    module._parameters[name] = new_param  # pylint: disable=protected-access # noqa: E501

        update(self.module)

    def zero_grad(self):
        """Sets gradients of all model parameters to zero."""
        for param in self.module.parameters():
            if param.grad is not None:
                param.grad.detach_()
                param.grad.zero_()

    def set_grads_none(self):
        """Sets gradients for all model parameters to None.

        This is an alternative to `zero_grad` which sets
        gradients to zero.
        """
        for param in self.module.parameters():
            if param.grad is not None:
                param.grad = None
